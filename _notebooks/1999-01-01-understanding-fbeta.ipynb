{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQQTtzM6u7+6knsu3uAlmU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTFrBnKuwK4A",
        "colab_type": "text"
      },
      "source": [
        "# Understanding the F-Beta metric\n",
        "> \"What is F-Beta, how should I use it, and what in the hell is 'average' and 'sample_weight'\"\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- image: images/some_folder/your_image.png\n",
        "- hide: true\n",
        "- search_exclude: false\n",
        "- show_tags: false\n",
        "- permalink: /temp-posts/fbeta-metric\n",
        "\n",
        "#- categories: [scikit-learn, metrics, multi-classification, multi-label]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26xjcj2SwaxZ",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html?highlight=fbeta#sklearn.metrics.fbeta_score) describes the F-Beta score \"*as the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0*\" with the \"`beta` parameter [determining] the weight of recall in the combined score.\"  It is one of the most common metrics enlisted in demonstrating the performance of binary, multi-classification, and multi-label classifiers.\n",
        "\n",
        "**So what does all that mean?**\n",
        "\n",
        "In a nutshell, it says that this metric can be used to help you understand how good your classification model is based on the relative importance you ascribe to precision and recall in making that determination.  Common `beta` values are **0.5** (precision is king), **1** (precision and recall are equally important), and **2** (recall is king).\n",
        "\n",
        "When you look at the documentation, you'll notice there are several other interesting arguments you can pass into it. Two of the more mysterious ones being `average` and `sample_weight`.  Will explore what they mean how you may want to use them based on your dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQtaLFS18OBa",
        "colab_type": "text"
      },
      "source": [
        "The two metrics, along with other important terms, are described well in this post.  Let's imagine a multi-classification model that tries to determine whether a photo show a picture of a dog, cat, or bird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB12gb1Nx1at",
        "colab_type": "text"
      },
      "source": [
        "## Precision vs. Recall\n",
        "\n",
        "The two metrics, along with other important terms, are described really well in this post.  Let's imagine a multi-classification model that tries to determine whether a given photo is a picture of a dog, cat, or bird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZVacPu1flS",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "Definition: When your classifier predicted a label, how often was it correct?\n",
        "\n",
        "Example: When you predicted 'cat', how often were you right? \n",
        "\n",
        "Formula: True Positive (TP) / ***PREDICTED*** Label (TP + False Positive or FP)\n",
        "```\n",
        "# TP = number of cat prediction you got right\n",
        "tp = 100\n",
        "# FP = number of cat predictions you got wrong\n",
        "fp = 10\n",
        "precision = tp / (tp + fp)\n",
        "# = 0.91\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDt_wh3D3put",
        "colab_type": "text"
      },
      "source": [
        "### Recall\n",
        "\n",
        "Definition: For every actual label in your dataset, how often did your classifier pick the correct one?\n",
        "\n",
        "Example: When it's actually 'cat', how often did it predict 'cat'?\n",
        "\n",
        "Formula: True Positive (TP) / ***ACTUAL*** Label (TP + False Negative or FN)\n",
        "```\n",
        "# TP = number of cat prediction you got right\n",
        "tp = 100\n",
        "# FN = number of actual cats you predicted as something else\n",
        "fn = 5\n",
        "recall = tp / (tp + fn)\n",
        "# = 0.95\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMqqHYo8Ge2",
        "colab_type": "text"
      },
      "source": [
        "### Okay, so which one should I use?\n",
        "\n",
        "This depends on your task.\n",
        "\n",
        "If you're task is to predict whether a patient has cancer given set of symptoms and test results, it's going to be far more important to you that all actual cancer patients get flagged even at the expense of non-cancer patients being flagged incorrectly.  *This is recall*.  In this particular kind of task, you're also likely going to be facing a dataset were the vast majority of examples are \"not cancer.\" A case where using metrics like precision and accuracy will likely look really good but be completely misleading.  Other examples where you want to maximize recall include fraud and network anomaly detection.\n",
        "\n",
        "On the otherhand, if you're task is to predict whether an e-mail is spam or not (1=spam|0=not spam), you recognize that it's not the end of the world if your user gets a junk e-mail.  If fact, it would be worse if an actual e-mail got flagged as junk and they didn't see it.  Getting it wrong is more acceptable than making sure all the true cases are gotten right.  *This is precision*.  Here, you're more concerned about your classifiers overall predictive capability in coming up with the right answer, yes or no.\n",
        "\n",
        "**What about our cats, dogs, birds?**\n",
        "\n",
        "Good question, again it depends on the task.  All things be equal, most likely we care more about precision or we care about both equally in this case.  Fortunately, the F-Beta metric gives us the power to determine the worth of our model regardless of how we want to weight the two.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JiAZrpA8tnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
