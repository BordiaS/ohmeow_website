{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Just testing updating from colab ... nice! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://ohmeow.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ohmeow.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding the F-Beta metric",
            "content": "Overview . scikit-learn describes the F-Beta score &quot;as the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0&quot; with the &quot;beta parameter [determining] the weight of recall in the combined score.&quot; It is one of the most common metrics enlisted in demonstrating the performance of binary, multi-classification, and multi-label classifiers. . So what does all that mean? . In a nutshell, it says that this metric can be used to help you understand how good your classification model is based on the relative importance you ascribe to precision and recall in making that determination. Common beta values are 0.5 (precision is king), 1 (precision and recall are equally important), and 2 (recall is king). . When you look at the documentation, you&#39;ll notice there are several other interesting arguments you can pass into it. Two of the more mysterious ones being average and sample_weight. Will explore what they mean how you may want to use them based on your dataset. . The two metrics, along with other important terms, are described well in this post. Let&#39;s imagine a multi-classification model that tries to determine whether a photo show a picture of a dog, cat, or bird. . Precision vs. Recall . The two metrics, along with other important terms, are described really well in this post. Let&#39;s imagine a multi-classification model that tries to determine whether a given photo is a picture of a dog, cat, or bird. . Precision . Definition: When your classifier predicted a label, how often was it correct? . Example: When you predicted &#39;cat&#39;, how often were you right? . Formula: True Positive (TP) / PREDICTED Label (TP + False Positive or FP) . # TP = number of cat prediction you got right tp = 100 # FP = number of cat predictions you got wrong fp = 10 precision = tp / (tp + fp) # = 0.91 . Recall . Definition: For every actual label in your dataset, how often did your classifier pick the correct one? . Example: When it&#39;s actually &#39;cat&#39;, how often did it predict &#39;cat&#39;? . Formula: True Positive (TP) / ACTUAL Label (TP + False Negative or FN) . # TP = number of cat prediction you got right tp = 100 # FN = number of actual cats you predicted as something else fn = 5 recall = tp / (tp + fn) # = 0.95 . Okay, so which one should I use? . This depends on your task. . If you&#39;re task is to predict whether a patient has cancer given set of symptoms and test results, it&#39;s going to be far more important to you that all actual cancer patients get flagged even at the expense of non-cancer patients being flagged incorrectly. This is recall. In this particular kind of task, you&#39;re also likely going to be facing a dataset were the vast majority of examples are &quot;not cancer.&quot; A case where using metrics like precision and accuracy will likely look really good but be completely misleading. Other examples where you want to maximize recall include fraud and network anomaly detection. . On the otherhand, if you&#39;re task is to predict whether an e-mail is spam or not (1=spam|0=not spam), you recognize that it&#39;s not the end of the world if your user gets a junk e-mail. If fact, it would be worse if an actual e-mail got flagged as junk and they didn&#39;t see it. Getting it wrong is more acceptable than making sure all the true cases are gotten right. This is precision. Here, you&#39;re more concerned about your classifiers overall predictive capability in coming up with the right answer, yes or no. . What about our cats, dogs, birds? . Good question, again it depends on the task. All things be equal, most likely we care more about precision or we care about both equally in this case. Fortunately, the F-Beta metric gives us the power to determine the worth of our model regardless of how we want to weight the two. .",
            "url": "https://ohmeow.com/temp-posts/fbeta-metric",
            "relUrl": "/temp-posts/fbeta-metric",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Cross Entropy Loss and You!",
            "content": ". # only run this cell if you are in collab !pip install git+https://github.com/fastai/fastai2 !pip install git+https://github.com/fastai/fastcore . Collecting git+https://github.com/fastai/fastai2 Cloning https://github.com/fastai/fastai2 to /tmp/pip-req-build-7ezen77p Running command git clone -q https://github.com/fastai/fastai2 /tmp/pip-req-build-7ezen77p Collecting fastcore Downloading https://files.pythonhosted.org/packages/3b/b5/aed836ce5b16ea1088a5d1a41d400bc051abf90bbef58bb74d8fd01a76af/fastcore-0.1.16-py3-none-any.whl Requirement already satisfied: torch&gt;=1.3.0 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (1.4.0) Requirement already satisfied: torchvision&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (0.5.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (3.2.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (1.0.3) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (2.21.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (3.13) Requirement already satisfied: fastprogress&gt;=0.1.22 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (0.2.2) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (7.0.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (0.22.2.post1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (1.4.1) Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (2.2.4) Requirement already satisfied: dataclasses&gt;=&#39;0.7&#39;; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from fastcore-&gt;fastai2==0.0.17) (0.7) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore-&gt;fastai2==0.0.17) (1.18.2) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision&gt;=0.5-&gt;fastai2==0.0.17) (1.12.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (1.1.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (2.4.6) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (2.8.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (0.10.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;fastai2==0.0.17) (2018.9) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (1.24.3) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (2019.11.28) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (2.8) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;fastai2==0.0.17) (0.14.1) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (2.0.3) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.0.2) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (7.4.0) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (0.4.1) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (4.38.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (0.6.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (46.0.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.0.2) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.1.3) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.0.0) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (3.0.2) Requirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai2==0.0.17) (1.6.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai2==0.0.17) (3.1.0) Building wheels for collected packages: fastai2 Building wheel for fastai2 (setup.py) ... done Created wheel for fastai2: filename=fastai2-0.0.17-cp36-none-any.whl size=186812 sha256=881c363bb8aeca2ebfd857a16dba4f29dd1fa7bf0f695735ec6a1e931c8a4707 Stored in directory: /tmp/pip-ephem-wheel-cache-uckpx78f/wheels/a1/59/9a/50335b36924b827e29d5f40b41fc3a008cc1f30dd80e560dfd Successfully built fastai2 Installing collected packages: fastcore, fastai2 Successfully installed fastai2-0.0.17 fastcore-0.1.16 Collecting git+https://github.com/fastai/fastcore Cloning https://github.com/fastai/fastcore to /tmp/pip-req-build-8855z7f_ Running command git clone -q https://github.com/fastai/fastcore /tmp/pip-req-build-8855z7f_ Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore==0.1.17) (1.18.2) Requirement already satisfied: dataclasses&gt;=&#39;0.7&#39; in /usr/local/lib/python3.6/dist-packages (from fastcore==0.1.17) (0.7) Building wheels for collected packages: fastcore Building wheel for fastcore (setup.py) ... done Created wheel for fastcore: filename=fastcore-0.1.17-cp36-none-any.whl size=28221 sha256=84f35b131cc447c57612d4e71a581c16a69c57ff33dcffdd48d287752933a9fe Stored in directory: /tmp/pip-ephem-wheel-cache-zf_v2cja/wheels/8a/2a/23/bc50c8f5e28776b44ac837a01fcfa675724565d4813d8e51c7 Successfully built fastcore Installing collected packages: fastcore Found existing installation: fastcore 0.1.16 Uninstalling fastcore-0.1.16: Successfully uninstalled fastcore-0.1.16 Successfully installed fastcore-0.1.17 . import torch from torch.nn import functional as F from fastai2.vision.all import * . We&#39;ve been doing multi-classification since week one, and last week, we learned about how a NN &quot;learns&quot; by evaluating its predictions as measured by something called a &quot;loss function.&quot; . So for multi-classification tasks, what is our loss function? . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.loss_func . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . FlattenedLoss of CrossEntropyLoss() . Negative Log-Likelihood &amp; CrossEntropy Loss . To understand CrossEntropyLoss, we need to first understand something called Negative Log-Likelihood . Negative Log-Likelihood (NLL) Loss . Let&#39;s imagine a model who&#39;s objective is to predict the label of an example given five possible classes to choose from. Out predictions might look like this ... . preds = torch.randn(3, 5); preds . tensor([[-0.3139, 0.6737, -0.0143, 1.9929, -0.6949], [ 0.5285, 0.1311, 0.2628, 0.6450, 1.7745], [-1.7458, 2.0199, -0.1365, 1.4622, -0.0940]]) . Because this is a supervised task, we know the actual labels of our three training examples above (e.g., the label of the first example is the first class, the label of the 2nd example the 4th class, and so forth) . targets = torch.tensor([0, 3, 4]) . Step 1: Convert the predictions for each example into probabilities using softmax. This describes how confident your model is in predicting what it belongs to respectively for each class . probs = F.softmax(preds, dim=1); probs . tensor([[0.0635, 0.1704, 0.0856, 0.6372, 0.0433], [0.1421, 0.0955, 0.1089, 0.1596, 0.4939], [0.0126, 0.5458, 0.0632, 0.3125, 0.0659]]) . If we sum the probabilities across each example, you&#39;ll see they add up to 1 . probs.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000]) . Step 2: Calculate the &quot;negative log likelihood&quot; for each example where y = the probability of the correct class . loss = -log(y) . We can do this in one-line using something called tensor/array indexing . example_idxs = range(len(preds)); example_idxs . range(0, 3) . correct_class_probs = probs[example_idxs, targets]; correct_class_probs . tensor([0.0635, 0.1596, 0.0659]) . nll = -torch.log(correct_class_probs); nll . tensor([2.7574, 1.8349, 2.7194]) . Step 3: The loss is the mean of the individual NLLs . nll.mean() . tensor(2.4372) . ... or using PyTorch . F.nll_loss(torch.log(probs), targets) . tensor(2.4372) . Cross Entropy Loss . ... or we can do this all at once using PyTorch&#39;s CrossEntropyLoss . F.cross_entropy(preds, targets) . tensor(2.4372) . As you can see, cross entropy loss simply combines the log_softmax operation with the negative log-likelihood loss . So why not use accuracy? . # this function is actually copied verbatim from the utils package in fastbook (see footnote 1) def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = torch.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . def f(x): return -torch.log(x) plot_function(f, &#39;x (prob correct class)&#39;, &#39;-log(x)&#39;, title=&#39;Negative Log-Likelihood&#39;, min=0, max=1) . NLL loss will be higher the smaller the probability of the correct class . What does this all mean? The lower the confidence it has in predicting the correct class, the higher the loss. It will: . 1) Penalize correct predictions that it isn&#39;t confident about more so than correct predictions it is very confident about. . 2) And vice-versa, it will penalize incorrect predictions it is very confident about more so than incorrect predictions it isn&#39;t very confident about . Why is this better than accuracy? . Because accuracy simply tells you whether you got it right or wrong (a 1 or a 0), whereast NLL incorporates the confidence as well. That information provides you&#39;re model with a much better insight w/r/t to how well it is really doing in a single number (INF to 0), resulting in gradients that the model can actually use! . Rember that a loss function returns a number. That&#39;s it! . Or the more technical explanation from fastbook: . &quot;The gradient of a function is its slope, or its steepness, which can be defined as rise over run -- that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths: (y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere. . As a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all!&quot; . Summary . So to summarize, accuracy is a great metric for human intutition but not so much for your your model. If you&#39;re doing multi-classification, your model will do much better with something that will provide it gradients it can actually use in improving your parameters, and that something is cross-entropy loss. . References . https://pytorch.org/docs/stable/nn.html#crossentropyloss | http://wiki.fast.ai/index.php/Log_Loss | https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ | https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy | https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/ |",
            "url": "https://ohmeow.com/temp-posts/understanding-cross-entropy-loss",
            "relUrl": "/temp-posts/understanding-cross-entropy-loss",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! My name is Wayde Gilliam, and I’m the owner of ohmeow.com. . In addition to being a full-stack web application and ML developer, I’m an active member in the fast.ai community and a contributor to their deep learning framework. I also have the privilege to mentor a number of High School students on a local FIRST Robotics FRC team in the ways of software development and analytics. . I’m a strong advocate of the thoughtful reading of books (not on a piece of glass mind you … the paper kind) and keeping a good work/life balance that promotes both intellectual and physical well-being (something I and most engineering type folks struggle with). I can honestly think of few things better for us as human beings to be involved with than the reading books. I’m not on most social media because I consider it a net negative to us as a species, however, you can find me on twitter where my account is primarily professional in nature or via e-mail. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ohmeow.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

}