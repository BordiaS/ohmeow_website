{
  
    
        "post0": {
            "title": "Cross Entropy Loss and You!",
            "content": "# only run this cell if you are in collab !pip install git+https://github.com/fastai/fastai2 !pip install git+https://github.com/fastai/fastcore . import torch from torch.nn import functional as F from fastai2.vision.all import * . We&#39;ve been doing multi-classification since week one, and last week, we learned about how a NN &quot;learns&quot; by evaluating its predictions as measured by something called a &quot;loss function.&quot; . So for multi-classification tasks, what is our loss function? . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.loss_func . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . FlattenedLoss of CrossEntropyLoss() . Negative Log-Likelihood &amp; CrossEntropy Loss . To understand CrossEntropyLoss, we need to first understand something called Negative Log-Likelihood . Negative Log-Likelihood (NLL) Loss . Let&#39;s imagine a model who&#39;s objective is to predict the label of an example given five possible classes to choose from. Out predictions might look like this ... . preds = torch.randn(3, 5); preds . tensor([[-0.3139, 0.6737, -0.0143, 1.9929, -0.6949], [ 0.5285, 0.1311, 0.2628, 0.6450, 1.7745], [-1.7458, 2.0199, -0.1365, 1.4622, -0.0940]]) . Because this is a supervised task, we know the actual labels of our three training examples above (e.g., the label of the first example is the first class, the label of the 2nd example the 4th class, and so forth) . targets = torch.tensor([0, 3, 4]) . Step 1: Convert the predictions for each example into probabilities using softmax. This describes how confident your model is in predicting what it belongs to respectively for each class . probs = F.softmax(preds, dim=1); probs . tensor([[0.0635, 0.1704, 0.0856, 0.6372, 0.0433], [0.1421, 0.0955, 0.1089, 0.1596, 0.4939], [0.0126, 0.5458, 0.0632, 0.3125, 0.0659]]) . If we sum the probabilities across each example, you&#39;ll see they add up to 1 . probs.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000]) . Step 2: Calculate the &quot;negative log likelihood&quot; for each example where y = the probability of the correct class . loss = -log(y) . We can do this in one-line using something called tensor/array indexing . example_idxs = range(len(preds)); example_idxs . range(0, 3) . correct_class_probs = probs[example_idxs, targets]; correct_class_probs . tensor([0.0635, 0.1596, 0.0659]) . nll = -torch.log(correct_class_probs); nll . tensor([2.7574, 1.8349, 2.7194]) . Step 3: The loss is the mean of the individual NLLs . nll.mean() . tensor(2.4372) . ... or using PyTorch . F.nll_loss(torch.log(probs), targets) . tensor(2.4372) . Cross Entropy Loss . ... or we can do this all at once using PyTorch&#39;s CrossEntropyLoss . F.cross_entropy(preds, targets) . tensor(2.4372) . As you can see, cross entropy loss simply combines the log_softmax operation with the negative log-likelihood loss . So why not use accuracy? . # this function is actually copied verbatim from the utils package in fastbook (see footnote 1) def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = torch.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . def f(x): return -torch.log(x) plot_function(f, &#39;x (prob correct class)&#39;, &#39;-log(x)&#39;, title=&#39;Negative Log-Likelihood&#39;, min=0, max=1) . NLL loss will be higher the smaller the probability of the correct class . What does this all mean? The lower the confidence it has in predicting the correct class, the higher the loss. It will: . 1) Penalize correct predictions that it isn&#39;t confident about more so than correct predictions it is very confident about. . 2) And vice-versa, it will penalize incorrect predictions it is very confident about more so than incorrect predictions it isn&#39;t very confident about . Why is this better than accuracy? . Because accuracy simply tells you whether you got it right or wrong (a 1 or a 0), whereast NLL incorporates the confidence as well. That information provides you&#39;re model with a much better insight w/r/t to how well it is really doing in a single number (INF to 0), resulting in gradients that the model can actually use! . Rember that a loss function returns a number. That&#39;s it! . Or the more technical explanation from fastbook: . &quot;The gradient of a function is its slope, or its steepness, which can be defined as rise over run -- that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths:(y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere. As a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all!&quot; 1 . Summary . So to summarize, accuracy is a great metric for human intutition but not so much for your your model. If you&#39;re doing multi-classification, your model will do much better with something that will provide it gradients it can actually use in improving your parameters, and that something is cross-entropy loss. . References . https://pytorch.org/docs/stable/nn.html#crossentropyloss | http://wiki.fast.ai/index.php/Log_Loss | https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ | https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy | https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/ | 1. fastbook chaper 4↩ .",
            "url": "https://ohmeow.com/2020/04/04/understanding-cross-entropy-loss.html",
            "relUrl": "/2020/04/04/understanding-cross-entropy-loss.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post3": {
            "title": "Cross Entropy Loss and You!",
            "content": "# only run this cell if you are in collab !pip install git+https://github.com/fastai/fastai2 !pip install git+https://github.com/fastai/fastcore . Collecting git+https://github.com/fastai/fastai2 Cloning https://github.com/fastai/fastai2 to /tmp/pip-req-build-tdvykzhg Running command git clone -q https://github.com/fastai/fastai2 /tmp/pip-req-build-tdvykzhg Collecting fastcore Downloading https://files.pythonhosted.org/packages/3b/b5/aed836ce5b16ea1088a5d1a41d400bc051abf90bbef58bb74d8fd01a76af/fastcore-0.1.16-py3-none-any.whl Requirement already satisfied: torch&gt;=1.3.0 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (1.4.0) Requirement already satisfied: torchvision&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (0.5.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (3.2.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (1.0.3) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (2.21.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (3.13) Requirement already satisfied: fastprogress&gt;=0.1.22 in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (0.2.2) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (7.0.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (0.22.2.post1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (1.4.1) Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai2==0.0.17) (2.2.4) Requirement already satisfied: dataclasses&gt;=&#39;0.7&#39;; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from fastcore-&gt;fastai2==0.0.17) (0.7) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore-&gt;fastai2==0.0.17) (1.18.2) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision&gt;=0.5-&gt;fastai2==0.0.17) (1.12.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (1.1.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (2.8.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2==0.0.17) (2.4.6) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;fastai2==0.0.17) (2018.9) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (1.24.3) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (2.8) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (2019.11.28) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2==0.0.17) (3.0.4) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;fastai2==0.0.17) (0.14.1) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (46.0.0) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.0.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (2.0.3) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.0.2) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (0.4.1) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.1.3) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (3.0.2) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (1.0.0) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (4.38.0) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (7.4.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2==0.0.17) (0.6.0) Requirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai2==0.0.17) (1.6.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai2==0.0.17) (3.1.0) Building wheels for collected packages: fastai2 Building wheel for fastai2 (setup.py) ... done Created wheel for fastai2: filename=fastai2-0.0.17-cp36-none-any.whl size=187332 sha256=0406ec6b4e8d06ac97a62fe6c37ecf7c5a9eda826960020516cbddbdd905929a Stored in directory: /tmp/pip-ephem-wheel-cache-snar4fxk/wheels/a1/59/9a/50335b36924b827e29d5f40b41fc3a008cc1f30dd80e560dfd Successfully built fastai2 Installing collected packages: fastcore, fastai2 Successfully installed fastai2-0.0.17 fastcore-0.1.16 Collecting git+https://github.com/fastai/fastcore Cloning https://github.com/fastai/fastcore to /tmp/pip-req-build-kuwf_6gx Running command git clone -q https://github.com/fastai/fastcore /tmp/pip-req-build-kuwf_6gx Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore==0.1.17) (1.18.2) Requirement already satisfied: dataclasses&gt;=&#39;0.7&#39; in /usr/local/lib/python3.6/dist-packages (from fastcore==0.1.17) (0.7) Building wheels for collected packages: fastcore Building wheel for fastcore (setup.py) ... done Created wheel for fastcore: filename=fastcore-0.1.17-cp36-none-any.whl size=28221 sha256=fa4a4a9581f1286d1ba070163240f2eb0a89012961a68e31d435ec8077dad96e Stored in directory: /tmp/pip-ephem-wheel-cache-vgateiyw/wheels/8a/2a/23/bc50c8f5e28776b44ac837a01fcfa675724565d4813d8e51c7 Successfully built fastcore Installing collected packages: fastcore Found existing installation: fastcore 0.1.16 Uninstalling fastcore-0.1.16: Successfully uninstalled fastcore-0.1.16 Successfully installed fastcore-0.1.17 . import torch from torch.nn import functional as F from fastai2.vision.all import * . We&#39;ve been doing multi-classification since week one, and last week, we learned about how a NN &quot;learns&quot; by evaluating its predictions as measured by something called a &quot;loss function.&quot; . So for multi-classification tasks, what is our loss function? . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.loss_func . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . FlattenedLoss of CrossEntropyLoss() . Negative Log-Likelihood &amp; CrossEntropy Loss . To understand CrossEntropyLoss, we need to first understand something called Negative Log-Likelihood . Negative Log-Likelihood (NLL) Loss . Let&#39;s imagine a model who&#39;s objective is to predict the label of an example given five possible classes to choose from. Out predictions might look like this ... . preds = torch.randn(3, 5); preds . tensor([[-0.3139, 0.6737, -0.0143, 1.9929, -0.6949], [ 0.5285, 0.1311, 0.2628, 0.6450, 1.7745], [-1.7458, 2.0199, -0.1365, 1.4622, -0.0940]]) . Because this is a supervised task, we know the actual labels of our three training examples above (e.g., the label of the first example is the first class, the label of the 2nd example the 4th class, and so forth) . targets = torch.tensor([0, 3, 4]) . Step 1: Convert the predictions for each example into probabilities using softmax. This describes how confident your model is in predicting what it belongs to respectively for each class . probs = F.softmax(preds, dim=1); probs . tensor([[0.0635, 0.1704, 0.0856, 0.6372, 0.0433], [0.1421, 0.0955, 0.1089, 0.1596, 0.4939], [0.0126, 0.5458, 0.0632, 0.3125, 0.0659]]) . If we sum the probabilities across each example, you&#39;ll see they add up to 1 . probs.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000]) . Step 2: Calculate the &quot;negative log likelihood&quot; for each example where y = the probability of the correct class . loss = -log(y) . We can do this in one-line using something called tensor/array indexing . example_idxs = range(len(preds)); example_idxs . range(0, 3) . correct_class_probs = probs[example_idxs, targets]; correct_class_probs . tensor([0.0635, 0.1596, 0.0659]) . nll = -torch.log(correct_class_probs); nll . tensor([2.7574, 1.8349, 2.7194]) . Step 3: The loss is the mean of the individual NLLs . nll.mean() . tensor(2.4372) . ... or using PyTorch . F.nll_loss(torch.log(probs), targets) . tensor(2.4372) . Cross Entropy Loss . ... or we can do this all at once using PyTorch&#39;s CrossEntropyLoss . F.cross_entropy(preds, targets) . tensor(2.4372) . As you can see, cross entropy loss simply combines the log_softmax operation with the negative log-likelihood loss . So why not use accuracy? . # this function is actually copied verbatim from the utils package in fastbook (see footnote 1) def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = torch.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . def f(x): return -torch.log(x) plot_function(f, &#39;x (prob correct class)&#39;, &#39;-log(x)&#39;, title=&#39;Negative Log-Likelihood&#39;, min=0, max=1) . NLL loss will be higher the smaller the probability of the correct class . What does this all mean? The lower the confidence it has in predicting the correct class, the higher the loss. It will: . 1) Penalize correct predictions that it isn&#39;t confident about more so than correct predictions it is very confident about. . 2) And vice-versa, it will penalize incorrect predictions it is very confident about more so than incorrect predictions it isn&#39;t very confident about . Why is this better than accuracy? . Because accuracy simply tells you whether you got it right or wrong (a 1 or a 0), whereast NLL incorporates the confidence as well. That information provides you&#39;re model with a much better insight w/r/t to how well it is really doing in a single number (INF to 0), resulting in gradients that the model can actually use! . Rember that a loss function returns a number. That&#39;s it! . Or the more technical explanation from fastbook: . &quot;The gradient of a function is its slope, or its steepness, which can be defined as rise over run -- that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths:(y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere. As a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all!&quot; 1 . Summary . So to summarize, accuracy is a great metric for human intutition but not so much for your your model. If you&#39;re doing multi-classification, your model will do much better with something that will provide it gradients it can actually use in improving your parameters, and that something is cross-entropy loss. . References . https://pytorch.org/docs/stable/nn.html#crossentropyloss | http://wiki.fast.ai/index.php/Log_Loss | https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ | https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy | https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/ | 1. fastbook chaper 4↩ .",
            "url": "https://ohmeow.com/multi-classification/2020/01/01/understanding-cross-entropy-loss.html",
            "relUrl": "/multi-classification/2020/01/01/understanding-cross-entropy-loss.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there! Wayde here. . I’m the owner of ohmeow.com and have been engaged in the development of mid-to-enterprise level application development for over 20 years. An active member in the fast.ai community and contributor to their deep learning framework, you can usually find me on their forums and/or tweeting about the latest and greatest from the world of AI. I also have the privilege to mentor a number of High School students on a local FIRST Robotics FRC team (go team 2102!). . I’m not one for most social media (honestly, most of it’s nonsense and a net negative to our species), however, you can find me on twitter where my account is primarily professional in nature or via e-mail. If you want to talk shop or see where we can help your organization, we’d love to hear from you! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ohmeow.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  

  
  

  
  

}