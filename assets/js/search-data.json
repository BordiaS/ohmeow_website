{
  
    
        "post0": {
            "title": "Text Generation with blurr",
            "content": "# only run this cell if you are in collab !pip install ohmeow-blurr !pip install nlp . Requirement already satisfied: ohmeow-blurr in /usr/local/lib/python3.6/dist-packages (0.0.5) Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from ohmeow-blurr) (0.0.12) Requirement already satisfied: fastai2 in /usr/local/lib/python3.6/dist-packages (from ohmeow-blurr) (0.0.17) Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from ohmeow-blurr) (2.10.0) Requirement already satisfied: fastcore in /usr/local/lib/python3.6/dist-packages (from ohmeow-blurr) (0.1.17) Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from ohmeow-blurr) (4.10.1) Requirement already satisfied: numpy&gt;=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval-&gt;ohmeow-blurr) (1.18.4) Requirement already satisfied: Keras&gt;=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval-&gt;ohmeow-blurr) (2.3.1) Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (2.2.4) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (3.2.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (1.0.3) Requirement already satisfied: fastprogress&gt;=0.1.22 in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (0.2.3) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (2.23.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (0.22.2.post1) Requirement already satisfied: torch&gt;=1.3.0 in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (1.5.0+cu101) Requirement already satisfied: torchvision&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (0.6.0+cu101) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (1.4.1) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (7.0.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai2-&gt;ohmeow-blurr) (3.13) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (4.41.1) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (0.7) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (2019.12.20) Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (0.0.43) Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (0.1.91) Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (0.7.0) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers-&gt;ohmeow-blurr) (3.0.12) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel-&gt;ohmeow-blurr) (4.5.3) Requirement already satisfied: traitlets&gt;=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel-&gt;ohmeow-blurr) (4.3.3) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel-&gt;ohmeow-blurr) (5.3.4) Requirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel-&gt;ohmeow-blurr) (5.5.0) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval-&gt;ohmeow-blurr) (2.10.0) Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval-&gt;ohmeow-blurr) (1.1.2) Requirement already satisfied: keras-applications&gt;=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval-&gt;ohmeow-blurr) (1.0.8) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval-&gt;ohmeow-blurr) (1.12.0) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (7.4.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (2.0.3) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (46.3.0) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (0.4.1) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (1.0.0) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (1.0.2) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (1.1.3) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (0.6.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (1.0.2) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai2-&gt;ohmeow-blurr) (3.0.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2-&gt;ohmeow-blurr) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2-&gt;ohmeow-blurr) (0.10.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2-&gt;ohmeow-blurr) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai2-&gt;ohmeow-blurr) (2.4.7) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;fastai2-&gt;ohmeow-blurr) (2018.9) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2-&gt;ohmeow-blurr) (2.9) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2-&gt;ohmeow-blurr) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2-&gt;ohmeow-blurr) (2020.4.5.1) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai2-&gt;ohmeow-blurr) (1.24.3) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;fastai2-&gt;ohmeow-blurr) (0.15.1) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.3.0-&gt;fastai2-&gt;ohmeow-blurr) (0.16.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers-&gt;ohmeow-blurr) (7.1.2) Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets&gt;=4.1.0-&gt;ipykernel-&gt;ohmeow-blurr) (4.4.2) Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets&gt;=4.1.0-&gt;ipykernel-&gt;ohmeow-blurr) (0.2.0) Requirement already satisfied: jupyter-core&gt;=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client-&gt;ipykernel-&gt;ohmeow-blurr) (4.6.3) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client-&gt;ipykernel-&gt;ohmeow-blurr) (19.0.1) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.6/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (0.8.1) Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (2.1.3) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (1.0.18) Requirement already satisfied: pexpect; sys_platform != &#34;win32&#34; in /usr/local/lib/python3.6/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (4.8.0) Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (0.7.5) Requirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai2-&gt;ohmeow-blurr) (1.6.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (0.1.9) Requirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != &#34;win32&#34;-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;ohmeow-blurr) (0.6.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai2-&gt;ohmeow-blurr) (3.1.0) Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.1.0) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1) Requirement already satisfied: pyarrow&gt;=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (0.17.1) Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.4) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12) Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.1.1) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;nlp) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;nlp) (2.9) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;nlp) (2020.4.5.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;nlp) (3.0.4) . import nlp import pandas as pd from fastai2.text.all import * from transformers import * from blurr.data.all import * from blurr.modeling.all import * . Data Preparation . We&#39;re going to use to use the new nlp library from huggingface to grab your raw data. This package gives you access to all kinds of NLP related datasets, explanations of each, and various task specific metrics to use in evaluating your model. The best part being everything comes down to you in JSON! This makes it a breeze to get up and running quickly! . raw_data = nlp.load_dataset(&#39;cnn_dailymail&#39;) raw_data.keys() . dict_keys([&#39;test&#39;, &#39;train&#39;, &#39;validation&#39;]) . We&#39;ll just use a subset of the training set to build both our training and validation DataLoaders . df = pd.DataFrame(raw_data[&#39;train&#39;]) df.head() . article highlights . 0 (CNN) -- A blowout preventer that may hold important forensic evidence as to why it failed, triggering the world&#39;s largest accidental oil spill, has been brought to the surface of the Gulf and placed on a vessel, officials said Saturday night. The device &quot;was taken into custody by the U.S. Department of Justice as evidence in its ongoing investigation into the incident,&quot; BP said. The blowout preventer was lifted to the surface at 8:53 p.m. (9:53 p.m. ET). Adm. Thad Allen, the government&#39;s national incident commander, said the huge blowout preventer &quot;is considered evidentiary material.&quot; The... | NEW: The failed blowout preventer is brought to the surface . nA new blowout preventer has been placed on the capped well . nBP will continue work on its Gulf relief well . | . 1 (CNN) -- A controversial photography exhibit called &quot;In the Playroom&quot; depicts young children reenacting tragic and violent historical events, including the September 11 attacks and the abuse of inmates at Iraq&#39;s infamous Abu Ghraib prison. The artist, Jonathan Hobin, says his work is an attempt to reflect on modern events that affect children and prompt dialogue about &quot;issues in our world.&quot; But the photos have also drawn criticism from those who say Hobin&#39;s use of artistic license involving children crosses an ethical boundary. &quot;Some of it is appropriate,&quot; said psychiatrist Alvin Poussaint... | Exhibit &quot;In the Playroom&quot; depicts young children re-enacting violent events . nPhotographer Jonathan Hobin says it reflects on modern events that affect children . nHe says the exhibit is supposed to prompt dialogue about &quot;issues in our world&quot; nThe photos have drawn criticism from those who say it crosses an ethical line . | . 2 (CNN)Wintry weather is a relative thing. The Northeast is on alert -- urging drivers to stay off roads -- as the fourth storm in three weeks drops snow that in Boston has exceeded 45 inches in February alone. It takes a fraction of that to get residents in the South into emergency mode. Sometimes, it doesn&#39;t even have to snow -- just the possibility of wintry weather is enough for a partial shutdown. &quot;Wintry precipitation possible&quot; early this week, the National Weather Service in Atlanta said. The Weather Service issued a winter storm watch for northern Georgia, warning of potential snow a... | Winter storm watch in effect for parts of the South . nJust a little snow or ice can be enough to cause havoc . | . 3 (CNN) -- U.S. Sgt. Bowe Bergdahl -- a former captive, a survivor and a hometown hero to many in Hailey, Idaho -- is being accused by some of his fellow soldiers of deserting his post. This charge of treason will be proven or dismissed in the coming weeks and months. But whether it is true or not, and whether he faces official military charges, he has already been sentenced to life to be haunted by his captivity -- and if the allegations are true, his choices. Survivors of the Hanoi Hilton, POWs during Vietnam such as Sen. John McCain and Medal of Honor winner Adm. James Stockdale, endured ... | Bowe Bergdahl a &quot;hometown hero&quot; in Hailey, Idaho, and a &quot;deserter&quot; to some soldiers . nO&#39;Shea: He learned Pashtun; building rapport with captors is critical to survival as a hostage . nHe says Bergdahl probably wondered each day if this would be his last . nTo break vow to fellow soldiers is terrible: If he indeed deserted, that will haunt him, too . | . 4 (CNN) -- Rafael Nadal continued his domination on clay as he crushed compatriot Guillermo Garcia-Lopez 6-1, 6-2 in the second round of the Barcelona Open. The Spaniard carved out 15 break points during the match, polishing off Garcia-Lopez in just over 80 minutes. It was Nadal&#39;s 30th straight victory at the event, where he has won six of the last seven titles at the Real Club de Tennis. Nadal has started the season in ominous form. After battling back from a serious knee injury, the world number two won his first title in ten months last week after finally beating rival -- and world number... | World number two Rafael Nadal starts with win at Barcelona Open . nReigning champion enjoys 30th straight victory in the tournament . nFormer world number one Ana Ivanovic loses to qualifier in Stuttgart . nGerman Mona Barthel watched Ivanovic win French Open whilst at school . | . We begin by getting our hugginface objects needed for this task (e.g., the architecture, tokenizer, config, and model). We&#39;ll use blurr&#39;s get_hf_objects helper method here. . pretrained_model_name = &quot;bart-large-cnn&quot; hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, BartTokenizer, HF_MODELS.BartForConditionalGeneration) hf_arch, type(hf_tokenizer), type(hf_config), type(hf_model) . (&#39;bart&#39;, transformers.tokenization_bart.BartTokenizer, transformers.configuration_bart.BartConfig, transformers.modeling_bart.BartForConditionalGeneration) . Next we need to build out our DataBlock. Remember tha a DataBlock is a blueprint describing how to move your raw data into something modelable. That blueprint is executed when we pass it a data source, which in our case, will be the DataFrame we created above. We&#39;ll use a random subset to get things moving along a bit faster for the demo as well. . Notice we&#39;re specifying trg_max_length to constrain our decoder inputs to 250 so that our input/predicted summaries will be padded to 250 rather than the default which is whatever you are using for your encoder inputs (e.g., the text you want summarized). . blocks = ( HF_TextBlock(hf_arch, hf_tokenizer), HF_TextBlock(hf_arch, hf_tokenizer, task=ForConditionalGenerationTask(), trg_max_length=250) ) dblock = DataBlock(blocks=blocks, get_x=ColReader(&#39;article&#39;), get_y=ColReader(&#39;highlights&#39;), splitter=RandomSubsetSplitter(0.05, 0.01)) . dls = dblock.dataloaders(df, bs=4) . It&#39;s always a good idea to check out a batch of data and make sure the shapes look right. . b = dls.one_batch() len(b), b[0][0].shape, b[1].shape . (2, torch.Size([4, 512]), torch.Size([4, 250])) . Even better, we can take advantage of blurr&#39;s TypeDispatched version of show_batch to look at things a bit more intuitively. . dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2) . text target . 0 Mauricio Pochettino might want to look up that old saying about houses built on sand. It might just ring a Christmas bell or two regarding a side who are reasonably capable of scoring but need some pointers on how to defend. That much was obvious here in a performance of numerous back-line errors that had to be redeemed by goals 85 minutes apart. Leading after four minutes through Harry Kane, Tottenham spent large swathes of what remained hanging on. Had Wilfried Bony made more of the space he was repeatedly afforded by Federico Fazio, this might have been a different result altogether. Tottenham Hotspur midfielder Christian Eriksen celebrates scoring an 89th-minute winner at the Liberty Stadium against Swansea City. Tottenham&#39;s Harry Kane (left), Eriksen (centre) and Jan Vertonghen (right) celebrate their 89th-minute winner on Sunday. Christian Eriksen (left) scores a late winner for Tottenham Hotspur as they took all three points away from the Liberty Stadium. Swansea City&#39;s (from left) Ashley Williams, Gylfi Sigurdsson, Jonjo Shelvey and Ki Sung-yueng look devastated after conceding late on. Harry Kane clinches his first in celebration after putting Tottenham Hotspur 1-0 ahead against Swansea City at the Liberty Stadium. Kane (centre) leaps above the Swansea defence to power a header into the back of the net in just the fourth minute of the game. The Tottenham forward clenches his fist in celebration and runs off into the corner after opening the scoring on Sunday. Kane (centre, obscured) is congratulated by team-mates after opening the scoring at the Liberty Stadium on Sunday. Instead, Bony scored only once and Tottenham rode a Swansea storm until Christian Eriksen picked their pocket at the other end. As Swansea manager Garry Monk put it: ‘It would have been stopped in the second half if it was a boxing match.’ Pochettino, meanwhile, was asked if there was a Spanish phrase for ‘smash and grab’, but he didn’t understand the question. Instead he focused on the ‘positives’, which in this case meant a first win in three league games and a tribute to the team’s spirit. Certainly, there was no shortage of application from Tottenham’s players when the going got tough and that has not always been the case in this strange season. SWANSEA CITY (4-2-3-1): Tremmel 6; | Harry Kane opens the scoring for Tottenham Hotspur with a header after only four minutes at the Liberty Stadium. nWilfried Bony equalises for Swansea City (48mins) on his 50th Premier League appearance for the club. nChristian Eriksen scores late winner for Spurs with a shot from just inside the penalty area. nSpurs move up to seventh in the table and leapfrog the Swans, who now sit ninth in the Premier League. | . 1 USA are on the verge of qualifying for the last-16 of the World Cup after a dramatic draw 2-2 with Portugal on Sunday. Jurgen Klinsmann&#39;s side had led the game until injury time before a last-gasp equaliser from Silvestre Varela delayed their passage into the second round. Either way it has been a superb tournament so far for the Americans, who also beat Ghana 2-1 in their opening game, now Sportsmail takes a look at the men who have taken them this far in Brazil. Soccer stars: USA are on the verge of qualification to the last-16 of the World Cup. Tim Howard – Everton. Born: New Jersey, March 6, 1979 (35) Clubs: North Jersey Imperials, Metrostars, Manchester United, Everton. Education: Raised in North Brunswick, one of the USA’s most successful exports in fact spent his formative years as a midfield player, balancing his talent for soccer with his obsession with basketball. Howard, affectionately known as Timmy by those who know him, was a teenage sensation on the court. His basketball coach Eddie Breheney recently told CBS: ‘Timmy’s athletic ability as a high school student was really off the charts. His agility, his hand-eye coordination, his stamina and just his love and passion for the game.’ By the age of 15, though, Howard was in goal for the USA youth teams and emerged as the nation’s most exciting young player, wining the MLS goalkeeper of the year award in 2001 and then earning a move to Manchester United in 2003. Value: £3million. All smiles: Tim Howard acknowledges the US fans after their 2-2 draw with Portugal. Geoff Cameron – Stoke City. Born: Massachusetts, 07/11/1985 (28) Clubs: Rhode Island Stingrays, Houston Dynamo, Stoke City. Education: After emerging into the MLS draft from the University of Rhode Island, Cameron was plucked out by Houston Dynamo. The versatile Stoke defender, a hero for many young soccer fans in the States, recently reflected on the rise of the game in an interview with the Providence Journal: ‘In Europe, in South America, in Africa — just about everywhere else in the world — most kids grow up playing soccer,’ he said. ‘But there are so many other sports in America. There’s the NFL, the NBA, MLB, the NHL, and they’re all full of great | USA are on the verge of qualifying for last 16 of the World Cup. nThey beat Ghana 2-1 in their opening game before drawing 2-2 with Portugal. nA point against Germany in their final game will secure qualification. | . Training . We&#39;ll prepare our BART model for training by wrapping it in blurr&#39;s HF_TextGenerationModelWrapper model object. This class will handle ensuring all our inputs get translated into the proper arguments needed by a huggingface conditional generation model. We&#39;ll also use a custom model splitter that will allow us to apply discriminative learning rates over the various layers in our huggingface model. . Once we have everything in place, we&#39;ll freeze our model so that only the last layer group&#39;s parameters of trainable. See here for our discriminitative learning rates work in fastai. . Note: This has been tested with BART only thus far (if you try any other conditional generation transformer models they may or may not work ... if you do, lmk either way) . model = HF_TextGenerationModelWrapper(hf_model) learn = Learner(dls, model, opt_func=ranger, loss_func=CrossEntropyLossFlat(ignore_index=hf_tokenizer.pad_token_id), cbs=[HF_BaseModelCallback], splitter=partial(text_gen_splitter, arch=hf_arch))#.to_fp16() learn.create_opt() learn.freeze() . It&#39;s also not a bad idea to run a batch through your model and make sure the shape of what goes in, and comes out, looks right. . b = dls.one_batch() preds = learn.model(b[0]) len(b), len(b[0]), b[0][0].shape, len(b[1]), b[1].shape, len(preds), preds[0].shape . (2, 4, torch.Size([4, 512]), 4, torch.Size([4, 250]), 2, torch.Size([4, 250, 50264])) . Still experimenting with how to use fastai&#39;s learning rate finder for these kinds of models. If you all have any suggestions or interesting insights to share, please let me know. We&#39;re only going to train the frozen model for one epoch for this demo, but feel free to progressively unfreeze the model and train the other layers to see if you can best my results below. . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=6.309573450380412e-07) . learn.fit_one_cycle(1, lr_max=3e-5) . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss time . . 41.72% [1497/3588 08:54&lt;12:26 3.0477] &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; And now we can look at the &quot;greedy decoded&quot; predictions ... . learn.show_results(hf_tokenizer=hf_tokenizer, max_n=2) . Even better though, blurr augments the fastai Learner with a generate_text method that allows you to use huggingface&#39;s PreTrainedModel.generate method to create something more human-like. . test_article = &quot;&quot; . outputs = learn.generate_text(test_article, early_stopping=True, num_beams=4, num_return_sequences=3) for idx, o in enumerate(outputs): print(f&#39;=== Prediction {idx+1} === n{o} n&#39;) . What about inference? Easy! . learn.export(fname=&#39;ft_cnndm_export.pkl&#39;) . inf_learn = load_learner(fname=&#39;ft_cnndm_export.pkl&#39;) inf_learn.generate_text(test_article) . That&#39;s it . blurr supports a number of huggingface transformer model tasks in addition to text generation (e.g., sequence classification , token classification, and question/answering). The docs include examples for each of these tasks if you&#39;re curious to learn more. . For more information about ohmeow or to get in contact with me, head over to ohmeow.com for all the details. . Thanks! . &lt;/div&gt; .",
            "url": "https://ohmeow.com/posts/2020/05/23/text-generation-with-blurr.html",
            "relUrl": "/posts/2020/05/23/text-generation-with-blurr.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
            "content": "# only run this cell if you are in collab # !pip install git+https://github.com/fastai/fastai2 # !pip install git+https://github.com/fastai/fastcore . from fastai2.vision.all import * . Intoduction . Flying at 50,000 feet . At a high level, most machine learning and deep learning systems can be summed up as consisting of three primary elements. Data, an architecture/model, and a loss function. It can be visually described as such: . . The data describes the information given to the model for learning a specific task, and the loss function provides the feedback necessary for the model to improve in that task via a number that tells it how well it is doing. . Why is thinking about our data pipeline important? . Simple! You can&#39;t have a good model without a good architecture and proper loss function, but you can&#39;t have anything without data. And getting good data that can be transformed into something modelable isn&#39;t necessarily easy. In the slide deck presentation heard throughout the ML world, Andrej Karpathy, Senior Director of Artifical Intelligence at Tesla, put it this way: . . Coming from academia and the utopia of prepared datasets ready of modeling, he found that in the real world, the bread and butter of a deep learning system and where the blood, sweat, and tears would be shed, was in the data. Data acquisition, cleaning, preparation, and the day-to-day management thereof. This same sentiment can as much be inferred from any of you that watched Jeremy Howard&#39;s v2 walk through in late 20191... every single session was about getting your data modelable using the new v2 bits. That should tell you a lot! . So how do we do it? How do we prepare our datasets for modeling? . While there are many ways, even with fast.ai, most indicators point to it&#39;s DataBlock API as the answer. . What is the DataBlock API? . The DataBlock API is a blueprint for transforming your raw data into something that can fed into a model using the fast.ai framework. It is their high-level data API, one that builds upon their low-level Datasets/DataLoaders API, and also their mid-level Transform based API. . All three incorporate some new ideas for getting your data good to go, and the choice isn&#39;t necessary one or the other. . Dropping down to 30,000 feet ... what is it? . The DataBlock API consists of THREE main components: getters, transforms, and a splitters. . getters tell it how to &quot;get&quot; the raw data (e.g., from the file system as file paths, a Pandas DataFrame). . | transforms tell it how to &quot;transform&quot; that raw data progressively into something that can be fed into a model (e.g., a numeric representation of your inputs and targets). . | splitters define various strategies you can implore to create your training and validation datasets. . | We&#39;ll be talking a lot about transforms in this article, but one of their most interesting characteristics is that they can be defined to transform your raw data into a numerical representation (as &quot;block transforms&quot;), to run on your CPU when an item from your dataset is fetched (as an &quot;item transform&quot;) , or on the GPU after a mini-batch of your data has been collated into a square matrix and right before it is ran through your model (as a &quot;batch transform&quot;). In fact, there are all kinds of hooks into the data processing pipeline whereby you can apply transforms! . An example . Let&#39;s break down one of the DataBlock examples from the documentation: . pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) . Your getters here are get_items and get_y. The first tells us that our inputs will be coming in the form of filenames returned by the get_image_files function, while the later tells the API to get the labels, or targets, for the inputs by applying a regex to the filename. There is also a get_x method available should you need to apply more specific instructions for defining your input. get_items, get_x, and get_y are all optional. Which you will need to implement depends on your data source and what you need to do to get your inputs/targets. . The splitter parameter tells us that we are going to randomly split this data with 80% for training and 20% for validation. How do I know this? Easy. In your notebook put whatever class/method you are interested followed by two ?? to see it&#39;s source. . RandomSplitter?? . So we got our data and we defined how we&#39;re going to split it for training/validation ... but how do we actually turn that into something we can feed a neural network? That is where transforms come into play and there are three primary kinds: . The data transforms defined in the blocks parameter describe how to &quot;transform&quot; your inputs and targets into what you really want to pass in to your model. Here we apply an ImageBlock to our inputs in order to turn the filenames into numerical representations of our images and a CategoryBlock to turn our targets from string labels to a unique set of numerical indexes for each of the possible labels. Essentially what these transforms do is turn your raw data into numbers because your data HAS to be represented numerically to train any kind of ML or DL model. | Next we define our item transforms via item_tfms. Our only item transform above will resize all our images to 128x128. We do this here because we&#39;ll need squared matrices to pass our images through our network in mini-batches (e.g., a subset of examples), and we can&#39;t create a mini-batch of items until they are all the same shape. These transforms are applied when we fetch an individual item from one of our datasets. | Lastly, we define our batch transforms via batch_tfms for transforms that will be applied to a &quot;mini-batch&quot; of data. Above we&#39;re saying, &quot;There&#39;s a bunch of cool data augmentations we want you to apply to the images in each mini-batch right before you send it through the model.&quot; Again, these transforms are applied on the GPU against a mini-batch of items. | You can apply transforms to a variety of places in the data processing loop, but these three will satisfy your needs 90-95% of the time. . Uh, okay ... so where&#39;s the data? . Remember that the pets DataBlock is just a blueprint, a pipeline for making raw data into modelable data. How do we build something based on this blueprint? Easy. We just call our DataBlock&#39;s dataloaders() method, passing in the one argument our get_items function, get_image_files, needs ... the directory path all your images files are under. . dls = pets.dataloaders(path/&quot;images&quot;) . Once your pets DataBlock knows the &quot;source&quot; of your data, it goes to work. It gets your image filenames, derives each image&#39;s label from that name, creates a training and validation dataset, and then applies the appropirate transforms, at the appropriate time, so that when you pull items from your DataLoaders object (your dls variable), you have something your model understands. This is the object you pass into your Learner to do the actual training. . Here&#39;s some code you can run yourself in colab: . path = untar_data(URLs.PETS) # &lt;-- Download our data; returns the path to that data pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) dls = pets.dataloaders(path/&quot;images&quot;) # &lt;-- Tell our DataBlock where the &quot;source&quot; is and off it goes dls.show_batch(max_n=9) # dls.valid.show_batch() . The Basics - PyTorch Datasets &amp; Dataloaders . Using the DataBlock API seems magical (well, it kinda is). We&#39;ve seen how easy it is to build this DataLoaders object that can be used to train our models, but in order to see what it is actually going on, we need to start at the beginning, we need to see how this is done natively in PyTorch. . Don&#39;t get confused by the similar concepts and names (e.g., Datasets, DataLoaders, transforms, etc...). Many of these ideas are built into PyTorch and extended to do much more in fast.ai. Just remember ... we&#39;re only working with PyTorch right now. . PyTorch itself provides Dataset and DataLoader classes for getting at our data and being able to iteratively run it through our model via mini-batches. Let&#39;s see how! . Dataset . A Pytorch Dataset (see torch.utils.data.Dataset) is defined as &quot;an abstract class representing a dataset&quot;2. That&#39;s just a fancy way to say it represents a collection of our data. We inherit from it and implement two key methods: . __len__: To return the size of our dataset . __getitem__: To get at a particular item in our dataset. . Let&#39;s start breaking down our DataBlock above by converting the underlying data representation as one of these Dataset classes. We&#39;ll import some new packages that will be using and create a PetCategories class that will allow us to map our target labels with their indexes (and vice-versa). . import pdb, re from torchvision import transforms . class PetCategories(): def __init__(self, image_fpaths, lbl_regex): # not all things are images self.lbl_regex = re.compile(lbl_regex) fpaths = [ f for f in image_fpaths if self.lbl_regex .match(f.name) ] # build our vocab self.vocab = dict(enumerate(set([self.lbl_regex.match(f.name).groups(0)[0] for f in fpaths if self.lbl_regex.match(f.name) ]))) # build a reverse lookup self.o2i = L(self.vocab.values()).val2idx() def get_label(self, fname): return self.lbl_regex.match(fname).groups(0)[0] . class PetsDataset(torch.utils.data.Dataset): def __init__(self, image_fpaths, pet_categories, item_tfms=None): # not all things are images self.fpaths = [ f for f in image_fpaths if f.name.endswith(&#39;.jpg&#39;)] # our &quot;item transforms&quot; self.tfm_pipeline = item_tfms # our labels vocab self.pet_categories = pet_categories def __len__(self): return len(self.fpaths) def __getitem__(self, idx): img_fpath = self.fpaths[idx] img_label = self.pet_categories.get_label(img_fpath.name) # you can think of this as a &quot;block&quot; or an &quot;data transform&quot; img = Image.open(img_fpath) lbl_idx = self.pet_categories.o2i[img_label] if self.tfm_pipeline: img = self.tfm_pipeline(img) return img, torch.tensor(lbl_idx) . There is a lot for you to explore above (step through the code, riddle it with pdb.set_trace statements, change it up and see what happens, etc....), but note the following in particular: . __getitem__ needs to return an &quot;example&quot;, which is two things ... your inputs/targets and they both need to be tensors. | item_tfms represents the PyTorch (not fast.ai) transforms we need to apply to our inputs/targets. We&#39;re going to use a special class named Compose from torchvision to set these up. For now, these transforms will just make sure our images are resized to the same size and converted to a tensor. Again, there is nothing fast.ai here (with the exception of me using the L class) ... we&#39;re just dealing with PyTorch righ now. :) | Notice how we have to create our own vocab and o2i method so we can return an integer representing the &quot;category&quot; rather than the category name (e.g. &quot;Maine_Coon&quot;) itself. Everything has to be a number! | TIP: Run all this code in colab ... do it! Make sure you understand what is going on and why. One of the most valuable techniques I use for learning all things Python, PyTorch, and fast.ai, is using pdb.set_trace() to step through and debug code. It&#39;s great way to build inutition by printing out the shapes of tensors, running parts of the code interactively, etc.... . Now ...we&#39;re going to need TWO Datasets ... one for training and one for validation. We&#39;ll split our examples up randomly and set aside 20% for our validation set. There&#39;s many ways to do this (most better and more efficient that below). . all_images = (path/&#39;images&#39;).ls(); len(all_images) . 7393 . rnd_idxs = np.random.permutation(len(all_images)); len(rnd_idxs) . 7393 . cut = int(len(rnd_idxs) * .2); cut . 1478 . train_idxs, valid_idxs = rnd_idxs[cut:], rnd_idxs[:cut] print(len(train_idxs), len(valid_idxs), len(train_idxs) + len(valid_idxs)) . 5915 1478 7393 . TIP: Notice how I print out lengths and shapes of tensors as I go? Doing that provides both a sanity check and ensure you are seeing what you expect before going further down the rabbit hole. . Now, we can create our training and validation Datasets. . Again, we are NOT using fast.ai transforms here ... these are all built into the torchvision package. They serve the same purpose here as the fast.ai &quot;item transforms&quot;, but for now, we&#39;re doing this all using just the PyTorch bits. . item_tfms = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.ToTensor() ]) . categories = PetCategories(all_images[train_idxs], lbl_regex=r&#39;^(.*)_ d+.jpg$&#39;) len(categories.vocab) . 37 . train_ds = PetsDataset(all_images[train_idxs], pet_categories=categories, item_tfms=item_tfms) valid_ds = PetsDataset(all_images[valid_idxs], pet_categories=categories, item_tfms=item_tfms) print(len(train_ds), len(valid_ds)) print(train_ds[20][0].shape, train_ds[20][1]) . 5913 1477 torch.Size([3, 224, 224]) tensor(33) . DataLoader . With that we can create a torch.utils.data.DataLoader from each Dataset. The primary reason we need this object is to yield mini-batches of data into our model, but as you can see, it also provides us the ability to do much more (e.g., shuffle data, provide a collate function, etc...). Check out the docs for more info! . Note: fast.ai has it&#39;s own DataLoader class that extends THIS one from PyTorch. Yah, I know it can seem confusing, but just remember for now, we are only working with functionality built-in to PyTorch. . bsz = 64 train_dl = torch.utils.data.DataLoader(train_ds, batch_size=bsz, shuffle=True) valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=bsz*2, shuffle=False) . And voila, we can now iterate through our dataset, mini-batch by mini-batch . b = next(iter(valid_dl)) len(b), b[0].shape, b[1].shape . (2, torch.Size([128, 3, 224, 224]), torch.Size([128])) . Wow ... that took quite a bit more work than the 6 lines of code to create a DataBlock, and it&#39;s still not as functional. For example, we haven&#39;t built anything that can decode items, show batches, or allow us to easily adjust/extend the objects we created above. . So let&#39;s keep going. Starting with the low-level API, we can take these PyTorch Dataset and DataLoader objects more friendly for fast.ai Learners. . Using the Low-Level API - fast.ai DataLoaders . It&#39;s actually pretty easy to get your PyTorch Dataset class incorporated into fast.ai and get it to play nicely with fast.ai&#39;s custom DataLoaders. . from fastai2.data.core import DataLoaders . dls = DataLoaders.from_dsets(train_ds, valid_ds) . b = dls.one_batch() . len(b), b[0].shape, b[1].shape . (2, torch.Size([64, 3, 224, 224]), torch.Size([64])) . I told you it was simple, didn&#39;t I? . Notice that we didn&#39;t have to change anything in our PyTorch Dataset to create a DataLoaders object we can pass to our Learner for training. This is nice because it means, given a standard PyTorch Dataset, you can use all the wonderful fast.ai bits for training in less than 3 lines of code. . Tip: If you don&#39;t care about being able to show batches, show results, and this satisfies your needs ... STOP! You&#39;re good to go. Don&#39;t overthink you&#39;re problem or over-engineer a solution to a problem that doesn&#39;t necessarily exist. Remember: You don&#39;t have to use the mid-level API or DataBlocks to use fast.ai! . Using the Mid-Level API - Converting Your Dataset into a Transform . BUT what if we want to apply/change our transforms, or run transforms on the GPU after we have a batch, or be able to visualize our data in our datasets and dataloaders or even our predictions? To begin with, we can convert our Dataset into a Transform by doing 4 things: . Inherit from Transform instead of torch.utils.data.Dataset | Change your __getitem__ into encodes. According to the docs ... &quot;a Transform in fastai calls the encodes method when you apply it on an item (a bit like PyTorch modules call forward when applied on something).&quot;3 Here it will return the numerical representations of our data in the form of tensors. | Change your return type to be a tuple and optionally use fastai&#39;s semantic types (here we wrap our image in TensorImage which knows how to show itself). From the docs: &quot;If you then return a tuple (or a subclass of a tuple), and use fastai&#39;s semantic type, you can then apply any other fastai&#39;s transform on your data and it will be dispatched properly.&quot;4 That simply means we can add on more transforms that know how to work with TensorImage objects and they&#39;ll do the right thing. | Get rid of __len__ | class PetsTransform(Transform): def __init__(self, image_fpaths, pet_categories, item_tfms=None): # not all things are images self.fpaths = [ f for f in all_images if f.name.endswith(&#39;.jpg&#39;)] # our pytorch &quot;item transforms&quot; self.tfm_pipeline = item_tfms # our labels vocab self.pet_categories = pet_categories def __len__(self): return len(self.fpaths) def encodes(self, idx): img_fpath = self.fpaths[idx] img_label = self.pet_categories.get_label(img_fpath.name) # you can think of this as a &quot;block&quot; or an &quot;data transform&quot; img = Image.open(img_fpath) lbl_idx = self.pet_categories.o2i[img_label] if self.tfm_pipeline: img = self.tfm_pipeline(img) return (TensorImage(img), torch.tensor(lbl_idx)) . Now that we are using a Transform, we have to use a new kind of object to build our dataset: TfmdLists . A TfmdList is &quot;just an object that lazily applies a collection of Transforms on a list.&quot;5 Think of it as a fancy Dataset object that knows how to work with Transform objects. . train_fpaths = all_images[train_idxs] valid_fpaths = all_images[valid_idxs] train_tl= TfmdLists(range(len(train_idxs)), PetsTransform(train_fpaths, pet_categories=categories, item_tfms=item_tfms)) valid_tl= TfmdLists(range(len(valid_idxs)), PetsTransform(valid_fpaths, pet_categories=categories, item_tfms=item_tfms)) . Since this is just another kind of dataset, we can pass these TfmdLists objects to DataLoaders just like before. But notice, we can now add fast.ai transforms to it just like we did in the DataBlock example at the top. We&#39;re already resizing and converting the examples to tensors, so we&#39;ll add some after_batch transforms for normalization and augmentations. . dls = DataLoaders.from_dsets(train_tl, valid_tl, after_batch=[Normalize.from_stats(*imagenet_stats), *aug_transforms()]) dls = dls.cuda() . b = dls.one_batch() len(b), b[0].shape, b[1].shape . (2, torch.Size([64, 3, 224, 224]), torch.Size([64])) . Let&#39;s see if we can show a batch of our data. Uncomment the line below, run it, and yah ... it throws an exception. But why? . # dls.show_batch() . If you guessed it is because show_batch doesn&#39;t know what to do with the target&#39;s numerical index, bingo! You&#39;re right. . Let&#39;s start to fix that by actually creating our own class that represents our inputs/targets. Notice that besides inheriting from Tuple, all we are providing is a show method that tells a PetImage object how to show itself. According to the docs, &quot;fastai will call [your transforms decodes methods] until it arrives at a type that knows how to show itself, then call the show method on this type.&quot;6 . BTW, a lot of this code is just ripped from the &quot;Siamese tutorial&quot; in the docs, so don&#39;t be too impressed. If you want to really do a deep dive and work though all this given a different task, check it out here. . class PetImage(Tuple): def show(self, ctx=None, **kwargs): img, category_idx = self if not isinstance(img, Tensor): img_tensor = tensor(img) img_tensor = img_tensor.permute(2,0,1) else: img_tensor = img return show_image(img_tensor, title=categories.vocab[category_idx], ctx=ctx, **kwargs) . The show method knows how to work with tensors or PIL images. The last method is a helper method available in fast.ai to actually show an image and print it&#39;s title above it. If you pass in a ctx it will use that to format and place the images appropriate. A context can be something like a matplotlib axis or a DataFrame ... it &quot;represents the object where we will show our thing.&quot;7 . Now let&#39;s make some changes to our PetsTransform to make it a bit more fastai&#39;sh. . First, we&#39;ll use PILImage.create to create the image in encodes. We do this because that object allows us to apply fast.ai transform liks Resize and ToTensor directly on it. . Second, we&#39;re going to move to using fast.ai transforms for everything, so we&#39;ll get rid of the PyTorch transforms! . Third, notice our encodes now returns a PetsImage. It&#39;s just a tuple ... but because its a particular kind of tuple, we can use the typdispatched show_batch and show_results to actually visualize our data/results. . class PetsTransform2(Transform): def __init__(self, image_fpaths, pet_categories): # not all things are images self.fpaths = [ f for f in all_images if f.name.endswith(&#39;.jpg&#39;)] # our labels vocab self.pet_categories = pet_categories def __len__(self): return len(self.fpaths) def encodes(self, img_fpath): img = PILImage.create(img_fpath) img_label = self.pet_categories.get_label(img_fpath.name) lbl_idx = self.pet_categories.o2i[img_label] return PetImage(img, lbl_idx) . Because of these changes, instead of creating the separate TfmdLists ourselves, we can now further do things the &quot;fast.ai way&quot; by using a splitter to do that for us. Here we&#39;ll use RandomSplitter which gives us that same 80/20 training/validation split. . splits = RandomSplitter()(all_images) tfm = PetsTransform2(all_images, categories) . Now we can get both our datasets in one line of code! When we pass splits to TfmdLists, it takes care of creating our training and validation datasets! . tls = TfmdLists(all_images, tfm, splits=splits) . And thanks for our PetImage class, fast.ai can show an item from our dataset. . show_at(tls.valid, 0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee17e84518&gt; . Even better, we can now specify all our transforms using fast.ai in the call to dataloaders(). And because these are fast.ai DataLoader objects, we can add tranforms at any point in our data processing pipeline (not just after_item and after_batch). . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . In the process, notice how we&#39;ve also refactored our code into something much more reusable. For example, if we want to resize our images to something else, its as easy as ... . new_dl = dls.new(after_item=[Resize(64), ToTensor]) new_dl.one_batch()[0].shape . torch.Size([64, 3, 64, 64]) . And what about showing a batch of data? Unfortunately it still won&#39;t work. show_batch is designed primarily to work with the DataBlock API, but here, we&#39;re returning the whole thing as a single transform. . The solution is easy: use the @typedispatch mechanism and override show_batch so that our x (our input) is &quot;typed&quot;. . b = dls.one_batch() . dls._types, type(b) . ({__main__.PetImage: [fastai2.torch_core.TensorImage, torch.Tensor]}, __main__.PetImage) . @typedispatch def show_batch(x:PetImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=3, figsize=None, **kwargs): if figsize is None: figsize = (ncols*6, max_n//ncols * 3) if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize) for i,ctx in enumerate(ctxs): PetImage(x[0][i], x[1][i].item()).show(ctx=ctx) . dls.show_batch() . When dls.show_batch() runs, it will find the closes matching version of show_batch() available to execute given chat the batch is. We could even write a typedispatched show_results() to look at our predictions alongside our targets using the same technique we applied to show_batch(). . Using the mid-level API, you not only have a Dataloaders object good to go for training ... you have one that you can use to show your data and extend by applying/changing as many transforms to wherever you want in the data processing pipeline. . What could be better than this? . Answer: Doing all this with &lt; 10 lines of code using the DataBlock API. . We&#39;ve already looked at how it works above, now, we&#39;ll look at the questions you need to ask to construct it in accordance with your data and task. Again, if the above gets you where you need to be, you don&#39;t need to use the high-level DataBlock API. There is no right option for every task and there are many ways to get where you need to go. . Using the High-Level API - DataBlocks . Having looked at the basic data-processing units in PyTorch, then to the low and mid-level APIs available in fast.ai, you&#39;re probably wondering, &quot;Ok, how can I do all that by drawing up a DataBlock blueprint for my task?&quot; . The path to enlightment comes in the form of answering 7 questions. . Asking the right questions . Assuming you understand your task and data, once you&#39;ve answered these 7 questions you&#39;ll know everything you need to construct your own DataBlock. These come right out of the DataBlock tutorial so check that for even more details and example implementations! . What are the types of your inputs and targets? (e.g., images/categories) | Where is your data? (e.g., filenames in folders, a DataFrame, a database) | Do we need to do anything special to get our &quot;inputs&quot;? If so, use get_x | Do we need to do anything special to get our &quot;targets&quot;? If yes, use get_y | How do you want to split the data into training and validation sets? Use splitter | Do we need to do anything when we get an item? If yes, define that in item_tfms | Do we need to do anything to a &quot;mini-batch&quot; of data? If yes, define that in batch_tfms | Getting the right answers . Looking back at our example DataBlock ... . pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) . We knew how to construct it as such because: . 1.What are the types of your inputs and targets? Answer: inputs=pet images | targets=37 categories. So we need an ImageBlock to handle the images and a CategoryBlock to handle the labels. Those blocks will add the needed transforms for each of their respective pieces. . 2.Where is your data? Answer: filenames . 3.Do we need to do anything special to get our &quot;inputs&quot;? Answer: No, get_items will get our input images. . 4.Do we need to do anything special to get our &quot;targets&quot;? Answer: Yes, we need to implement a get_y to get our labels from the image file name. . 5.How do you want to split the data into training and validation sets? Answer: We just want a random 80/20 split, so use RandomSplitter . 6.Do we need to do anything when we get an item? Answer: Yes, we need to resize our images so they are the same shape and can be included together in a mini-batch. Do this in item_tfms . 7.Do we need to do anything to a &quot;mini-batch&quot; of data? Answer: Yes, we&#39;d like to add some randomization to the images by applying data augmentations on the GPU. Do this with batch_tfms . Tips, Tricks, Best Practices, &amp; A Bunch of Good Things to Know . Below are some of the more important things and best practices to be aware of when working with the DataBlock API. It&#39;s in no way exhaustive, but anything I&#39;ve had to lookup multiple times is listed here. . What happens if I don&#39;t define how to get my targets (my y)? . If you don&#39;t specify your labels, the DataBlock API will assume they are the same as your inputs. This is atypical for most tasks, but not entirely useless. According to the docs, &quot;by default, the data block API assumes we have an input and a target, which is why we see our filename repeated twice&quot; whenever you view the results of your datasets/dataloaders without a y specified.8 . Can I have multiple inputs/targets? . Yes! According to the docs ... &quot;You can also have more than two blocks (if you have multiple inputs and/or targets), you would just need to pass n_inp to the DataBlock to tell the library how many inputs there are (the rest would be targets) and pass a list of functions to get_x and/or get_y (to explain how to process each item to be ready for his type).&quot;9 We&#39;ll explore this in Part 2 of this series where I attempt to update my v1 MixedTabluarList object (incorporates tabular + text) into something v2 friendly. In the meantime, here&#39;s a nice example from the docs on setting up a dataset for object detection: . coco = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=[lambda o: img2bbox[o.name][0], lambda o: img2bbox[o.name][1]], item_tfms=Resize(128), batch_tfms=aug_transforms(), n_inp=1) . You see that n_inp? It&#39;s saying, &quot;Use the ImageBlock for my inputs (I only have 1), but I&#39;ll need TWO targets this time as I&#39;m trying to predict the location of an object (BBoxBlock) and it&#39;s label (BBoxLblBlock).&quot; Notice also because we are predicting TWO things, our get_y returns a list of, you guessed it, two things. If we didn&#39;t need to do anything special with either of these targets, we&#39;d simply pass noop in it&#39;s place in that list. . Where can I learn about the baked in bits of the DataBlock API? . The API already has a lot of useful classes and functions suitable for defining your getters, splitter, and transforms across a number of application types. The full list is here: http://dev.fast.ai/data.transforms . What if something goes wrong? Or what if I want to make sure my DataBlock is doing what I think it is? . Use dblock.summary(path). If there is an error, this thing will bomb out where it is encountered ... else, you&#39;ll be able to verify that all the wonderful things your 5-10 lines of code above does what you expect. . Do I need to always use get_items? . No. For example, if your &quot;source&quot; data is a DataFrame ... . pascal = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=ColSplitter(), get_x=ColReader(0, pref=pascal_source/&quot;train&quot;), get_y=ColReader(1, label_delim=&#39; &#39;), item_tfms=Resize(224), batch_tfms=aug_transforms()) dls = pascal.dataloaders(df) . According to the docs ... &quot;we wont have to use a get_items function here because we already have all our data in one place.&quot;10 . What are different ways I can get my x and y from a DataFrame? . Using ColReader: . get_x=ColReader(0, pref=pascal_source/&quot;train&quot;), get_y=ColReader(1, label_delim=&#39; &#39;) . Using lambda functions: . get_x=lambda x:pascal_source/&quot;train&quot;/f&#39;{x[0]}&#39;, get_y=lambda x:x[1].split(&#39; &#39;), . Using column names: . get_x=lambda o:f&#39;{pascal_source}/train/&#39;+o.fname, get_y=lambda o:o.labels.split(), . Using from_columns: . def _pascal_items(x): return (f&#39;{pascal_source}/train/&#39;+x.fname, x.labels.str.split()) valid_idx = df[df[&#39;is_valid&#39;]].index.values pascal = DataBlock.from_columns(blocks=(ImageBlock, MultiCategoryBlock), get_items=_pascal_items, splitter=IndexSplitter(valid_idx), item_tfms=Resize(224), batch_tfms=aug_transforms()) . According to the docs, this is &quot;the most efficient way (to avoid iterating over the rows of the dataframe, which can take a long time) .... It will use get_items to convert the columns in numpy arrays. The drawback is that since we lose the dataframe after extracting the relevant columns, we can&#39;t use a ColSplitter anymore.&quot;11 . What about tabular data? . We&#39;ll explore the tabular bits in a later part, but as the docs say, the &quot;tabular data doesn&#39;t really use the data block API as it&#39;s relying on another API with TabularPandas for efficient preprocessing and batching.&quot;12 Of course, where there is a will, there is a way, and so we&#39;ll see a possible solution in Part 2 or 3 of this series :). . Summary . As the famous song goes, &quot;we&#39;ve only just begun ....&quot; In future installments we&#39;ll dig into more of the particulars of the entire fast.ai data stack, and see how we can use it to solve some &quot;out-of-the-box&quot; tasks. . In the meantime, the best way for you to get a better handle on what&#39;s what, is to mess around with the many examples found in the v2 documentation here. . References . http://dev.fast.ai/tutorial.datablock | http://dev.fast.ai/tutorial.siamese | http://dev.fast.ai/data.block | http://dev.fast.ai/data.transforms | fastai v2 walk-thru playlist | Zach Mueller&#39;s &quot;A Guided Walk-through of 2.0&quot;: Lesson 1 | 1. See full playlist here↩ . 2. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class↩ . 3. http://dev.fast.ai/tutorial.siamese#Using-the-mid-level-API↩ . 4. Ibid.↩ . 5. http://dev.fast.ai/tutorial.siamese#Using-the-mid-level-API↩ . 6. http://dev.fast.ai/tutorial.siamese#Making-show-work↩ . 7. Ibid.↩ . 8. http://dev.fast.ai/tutorial.datablock↩ . 9. Ibid.↩ . 10. Ibid.↩ . 11. Ibid.↩ . 12. Ibid.↩ .",
            "url": "https://ohmeow.com/posts/2020/04/11/finding-datablock-nirvana-part-1.html",
            "relUrl": "/posts/2020/04/11/finding-datablock-nirvana-part-1.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Cross Entropy Loss and You!",
            "content": "# only run this cell if you are in collab !pip install git+https://github.com/fastai/fastai2 !pip install git+https://github.com/fastai/fastcore . import torch from torch.nn import functional as F from fastai2.vision.all import * . We&#39;ve been doing multi-classification since week one, and last week, we learned about how a NN &quot;learns&quot; by evaluating its predictions as measured by something called a &quot;loss function.&quot; . So for multi-classification tasks, what is our loss function? . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.loss_func . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . FlattenedLoss of CrossEntropyLoss() . Negative Log-Likelihood &amp; CrossEntropy Loss . To understand CrossEntropyLoss, we need to first understand something called Negative Log-Likelihood . Negative Log-Likelihood (NLL) Loss . Let&#39;s imagine a model who&#39;s objective is to predict the label of an example given five possible classes to choose from. Our predictions might look like this ... . preds = torch.randn(3, 5); preds . tensor([[-0.3139, 0.6737, -0.0143, 1.9929, -0.6949], [ 0.5285, 0.1311, 0.2628, 0.6450, 1.7745], [-1.7458, 2.0199, -0.1365, 1.4622, -0.0940]]) . Because this is a supervised task, we know the actual labels of our three training examples above (e.g., the label of the first example is the first class, the label of the 2nd example the 4th class, and so forth) . targets = torch.tensor([0, 3, 4]) . Step 1: Convert the predictions for each example into probabilities using softmax. This describes how confident your model is in predicting what it belongs to respectively for each class . probs = F.softmax(preds, dim=1); probs . tensor([[0.0635, 0.1704, 0.0856, 0.6372, 0.0433], [0.1421, 0.0955, 0.1089, 0.1596, 0.4939], [0.0126, 0.5458, 0.0632, 0.3125, 0.0659]]) . If we sum the probabilities across each example, you&#39;ll see they add up to 1 . probs.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000]) . Step 2: Calculate the &quot;negative log likelihood&quot; for each example where y = the probability of the correct class . loss = -log(y) . We can do this in one-line using something called tensor/array indexing . example_idxs = range(len(preds)); example_idxs . range(0, 3) . correct_class_probs = probs[example_idxs, targets]; correct_class_probs . tensor([0.0635, 0.1596, 0.0659]) . nll = -torch.log(correct_class_probs); nll . tensor([2.7574, 1.8349, 2.7194]) . Step 3: The loss is the mean of the individual NLLs . nll.mean() . tensor(2.4372) . ... or using PyTorch . F.nll_loss(torch.log(probs), targets) . tensor(2.4372) . Cross Entropy Loss . ... or we can do this all at once using PyTorch&#39;s CrossEntropyLoss . F.cross_entropy(preds, targets) . tensor(2.4372) . As you can see, cross entropy loss simply combines the log_softmax operation with the negative log-likelihood loss . So why not use accuracy? . # this function is actually copied verbatim from the utils package in fastbook (see footnote 1) def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = torch.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . def f(x): return -torch.log(x) plot_function(f, &#39;x (prob correct class)&#39;, &#39;-log(x)&#39;, title=&#39;Negative Log-Likelihood&#39;, min=0, max=1) . NLL loss will be higher the smaller the probability of the correct class . What does this all mean? The lower the confidence it has in predicting the correct class, the higher the loss. It will: . 1) Penalize correct predictions that it isn&#39;t confident about more so than correct predictions it is very confident about. . 2) And vice-versa, it will penalize incorrect predictions it is very confident about more so than incorrect predictions it isn&#39;t very confident about . Why is this better than accuracy? . Because accuracy simply tells you whether you got it right or wrong (a 1 or a 0), whereast NLL incorporates the confidence as well. That information provides you&#39;re model with a much better insight w/r/t to how well it is really doing in a single number (INF to 0), resulting in gradients that the model can actually use! . Rember that a loss function returns a number. That&#39;s it! . Or the more technical explanation from fastbook: . &quot;The gradient of a function is its slope, or its steepness, which can be defined as rise over run -- that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths:(y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere. As a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all!&quot; 1 . Summary . So to summarize, accuracy is a great metric for human intutition but not so much for your your model. If you&#39;re doing multi-classification, your model will do much better with something that will provide it gradients it can actually use in improving your parameters, and that something is cross-entropy loss. . References . https://pytorch.org/docs/stable/nn.html#crossentropyloss | http://wiki.fast.ai/index.php/Log_Loss | https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ | https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy | https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/ | 1. fastbook chaper 4↩ .",
            "url": "https://ohmeow.com/posts/2020/04/04/understanding-cross-entropy-loss.html",
            "relUrl": "/posts/2020/04/04/understanding-cross-entropy-loss.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post5": {
            "title": "Understanding the F-Beta metric",
            "content": "Overview . scikit-learn describes the F-Beta score &quot;as the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0&quot; with the &quot;beta parameter [determining] the weight of recall in the combined score.&quot; It is one of the most common metrics enlisted in demonstrating the performance of binary, multi-classification, and multi-label classifiers. . So what does all that mean? . In a nutshell, it says that this metric can be used to help you understand how good your classification model is based on the relative importance you ascribe to precision and recall in making that determination. Common beta values are 0.5 (precision is king), 1 (precision and recall are equally important), and 2 (recall is king). . When you look at the documentation, you&#39;ll notice there are several other interesting arguments you can pass into it. Two of the more mysterious ones being average and sample_weight. Will explore what they mean how you may want to use them based on your dataset. . The two metrics, along with other important terms, are described well in this post. Let&#39;s imagine a multi-classification model that tries to determine whether a photo show a picture of a dog, cat, or bird. . Precision vs. Recall . The two metrics, along with other important terms, are described really well in this post. Let&#39;s imagine a multi-classification model that tries to determine whether a given photo is a picture of a dog, cat, or bird. . Precision . Definition: When your classifier predicted a label, how often was it correct? . Example: When you predicted &#39;cat&#39;, how often were you right? . Formula: True Positive (TP) / PREDICTED Label (TP + False Positive or FP) . # TP = number of cat prediction you got right tp = 100 # FP = number of cat predictions you got wrong fp = 10 precision = tp / (tp + fp) # = 0.91 . Recall . Definition: For every actual label in your dataset, how often did your classifier pick the correct one? . Example: When it&#39;s actually &#39;cat&#39;, how often did it predict &#39;cat&#39;? . Formula: True Positive (TP) / ACTUAL Label (TP + False Negative or FN) . # TP = number of cat prediction you got right tp = 100 # FN = number of actual cats you predicted as something else fn = 5 recall = tp / (tp + fn) # = 0.95 . Okay, so which one should I use? . This depends on your task. . If you&#39;re task is to predict whether a patient has cancer given set of symptoms and test results, it&#39;s going to be far more important to you that all actual cancer patients get flagged even at the expense of non-cancer patients being flagged incorrectly. This is recall. In this particular kind of task, you&#39;re also likely going to be facing a dataset were the vast majority of examples are &quot;not cancer.&quot; A case where using metrics like precision and accuracy will likely look really good but be completely misleading. Other examples where you want to maximize recall include fraud and network anomaly detection. . On the otherhand, if you&#39;re task is to predict whether an e-mail is spam or not (1=spam|0=not spam), you recognize that it&#39;s not the end of the world if your user gets a junk e-mail. If fact, it would be worse if an actual e-mail got flagged as junk and they didn&#39;t see it. Getting it wrong is more acceptable than making sure all the true cases are gotten right. This is precision. Here, you&#39;re more concerned about your classifiers overall predictive capability in coming up with the right answer, yes or no. . What about our cats, dogs, birds? . Good question, again it depends on the task. All things be equal, most likely we care more about precision or we care about both equally in this case. Fortunately, the F-Beta metric gives us the power to determine the worth of our model regardless of how we want to weight the two. .",
            "url": "https://ohmeow.com/temp-posts/fbeta-metric",
            "relUrl": "/temp-posts/fbeta-metric",
            "date": " • Jan 1, 1999"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there! Wayde here. . I’m the owner of ohmeow.com and have been engaged in the development of mid-to-enterprise level application development for over 20 years. An active member in the fast.ai community and contributor to their deep learning framework, you can usually find me on their forums and/or tweeting about the latest and greatest from the world of AI. I also have the privilege to mentor a number of High School students on a local FIRST Robotics FRC team (go team 2102!). . I’m not one for most social media (honestly, most of it’s nonsense and a net negative to our species), however, you can find me on twitter where my account is primarily professional in nature or via e-mail. If you want to talk shop or see where we can help your organization, we’d love to hear from you! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ohmeow.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  

  
  

  
  

}